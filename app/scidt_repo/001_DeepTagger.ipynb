{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dependencies\n",
    "\n",
    "Check that all necessary libraries are added to your local environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "#pip install keras-bert\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from util import read_passages, evaluate, make_folds, clean_words, test_f1, to_BIO, from_BIO, from_BIO_ind, arg2param\n",
    "\n",
    "import tensorflow as tf\n",
    "#config = tf.ConfigProto()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "#sess = tf.Session(config=config)\n",
    "sess = tf.compat.v1.Session()\n",
    "#import keras.backend as K\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "K.set_session(sess)\n",
    "from keras.activations import softmax\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping,LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "from crf import CRF\n",
    "from attention import TensorAttention\n",
    "from custom_layers import HigherOrderTimeDistributedDense\n",
    "from generator import BertDiscourseGenerator\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "import discourse_tagger_generator_bert\n",
    "# import bert\n",
    "# from transformers import AutoTokenizer, AutoModel \n",
    "#!pip install transformers\n",
    "# from transformers import pipeline\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#CHECK TENSORFLOW VERSION - Should be 2.3.0\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Training\n",
    "\n",
    "You can skip this step if using an already trained model. If training a new model follow the syntax below and pass any additional parameters as need (example: attention). Be sure to include a training and validation file.\n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train.txt --validation_file val_subset.txt --att_context LSTM_clause --bidirectional --save --maxseqlen 40 --maxclauselen 60 --batch_size 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples:\n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train_subset.txt --validation_file val_subset.txt --save --maxseqlen 40 --maxclauselen 60 \n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train_small.txt --validation_file val_small.txt --att_context LSTM_clause --save --maxseqlen 40 --maxclauselen 60 \n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train.txt --validation_file val_subset.txt --att_context LSTM_clause --bidirectional --save --maxseqlen 40 --maxclauselen 60 --batch_size 20  \n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train.txt --validation_file val_subset.txt --att_context LSTM_clause --bidirectional --save --maxseqlen 40 --maxclauselen 60 --batch_size 20\n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train.txt --validation_file val_subset.txt --att_context LSTM_clause --bidirectional --save --maxseqlen 40 --maxclauselen 60\n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train_subset.txt --validation_file val_subset.txt --save --maxseqlen 40 --maxclauselen 60 \n",
    "\n",
    "* %run discourse_tagger_generator_bert.py --repfile scibert_scivocab_uncased --train_file train.txt --validation_file train_subset.txt --save --maxseqlen 40 --maxclauselen 60 #This Works --use_attention --crf --save --maxseqlen 0 --maxclauselen 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Testing\n",
    "\n",
    "Pass your test file in the command below with the same parameters as the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att=False_cont=LSTM_clause_lstm=False_bi=True_crf=False\n",
      "Loaded model:\n",
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 40, 768)]         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 40, 768)           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 40, 300)           230700    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 40, 700)           1822800   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 40, 700)           0         \n",
      "_________________________________________________________________\n",
      "discourse (TimeDistributed)  (None, 40, 15)            10515     \n",
      "=================================================================\n",
      "Total params: 2,064,015\n",
      "Trainable params: 2,064,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded weights\n",
      "Loaded label index: {'none': 0, 'B_fact': 1, 'B_method': 2, 'B_implication': 3, 'B_result': 4, 'I_result': 5, 'I_implication': 6, 'B_goal': 7, 'B_problem': 8, 'B_hypothesis': 9, 'I_fact': 10, 'I_method': 11, 'I_problem': 12, 'I_hypothesis': 13, 'I_goal': 14}\n",
      "Predicting on file ../data/testFiles/test_example.txt\n",
      "maxseqlen 40\n",
      "Filtering data\n",
      "WARNING:tensorflow:From C:\\Projects\\Sp21-Project1scratch\\scidt_repo\\discourse_tagger_generator_bert.py:113: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n"
     ]
    }
   ],
   "source": [
    "#This example should run in under 1 minute. The output file will be placed in the 'predictions' folder.\n",
    "\n",
    "%run discourse_tagger_generator_bert.py --repfile ../models/scibert_scivocab_uncased --test_file ../data/testFiles/test_example.txt --att_context LSTM_clause --bidirectional --save --maxseqlen 40 --maxclauselen 60 --batch_size 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
